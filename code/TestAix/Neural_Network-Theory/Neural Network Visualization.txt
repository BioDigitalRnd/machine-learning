Weights and Bias:
Weights are value associated with the inputs that basically decides how much importance that particular input has on the desired output
Bias are a constant value that is added to the weight to offset the result 
(e.g. input = 1
	 weight = 0.2
	 bias = 2.0
Output = (input * weight + bias))

Output layer : taking input from one of the Hidden layer not from sensors but still output the same stuff
output = inputs[0] * Weights[0] + inputs[1] * Weights[1] + inputs[2] * Weights[2] + bias 

* shape = what the size is at that dimension
	Array:                  	     Shape:
		1 = [1,5,6,2]       		    (4,)
		
		  					Type: 1D array, Vector
		  					if simple list in python then its a 
		  					1D array in numpy and its a Vector in maths
		  		
	Array:              			Shape:
		lol =     [[1,5,6,2],		     (2, 4)
	(list of Lists) [3,2,1,3]]

							Type: 2D array, Matrix(Rectangular Array)
									  		
	Array:              			Shape:
		lolol =     [[[1,5,6,2],		(3, 2, 4)
	(list of Lists     [3,2,1,3]]
	 of lists)        [[5,2,1,2],
	 			   [6,4,8,4]],
	 			  [[2,8,5,3],
	 			   [1,1,9,4]]]
	
							Type: 3D array

Tensor Flow : an object that can be represented as an array

Input =  Vector
Weight = Vector
Bias = Vector

out = weight * input + bias similar to (as it is used to predict something)
y = mx + c
This is what's fed to the Rectified Linear Activation Function
Generally your hidden layers have a different activation function compared to your output layer
y = { x: x > 0  }
    { 0: x <= 0 }

* This would show the point that weights and biases, weights flipping the sign or biases offsetting enough in such a way
that you get this grandlier output

Sigmoid Functions occurs after the out function has been executed
We use a Sigmoid function instead of just the step function because we will not find out how close we are from 0 or 1
with the Step function so we use sigmoid as it is more reliable to train

+ Why We use Sigmoid Function ?
- if we just use weight and biases for our alogorithm y = x linear activation all the outputs would be linear
if we try to fit a linear activation function to a non linear function like a sine wave it wouldn't fit
- Rectified Linear Activation Function(ReLU) would fit better even with the same hidden layers and amount of neurons as it
is almost linear 

How does ReLU work:
- with a single neuron (increasing the weight will strengthen the input, then bias can offset the activation point moving it horizontally)
if we negate the weight we can flip the ReLU activation from determining how the input activates but instead how the input deactivates
- top 7 neuron pairs will be adjusted to fit the (sine)shape
whilst the bottom pair will solely work on offsetting the function
* That's why to fit non linear problems we need two or more hidden layers and with those two hidden layers that's why we use non linear 
activation functions

